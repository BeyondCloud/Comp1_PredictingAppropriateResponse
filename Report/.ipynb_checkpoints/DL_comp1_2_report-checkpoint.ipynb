{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning competition 1\n",
    "**Team 2**\n",
    "106061514, 許鈞棠 106061536, 廖學煒\n",
    "\n",
    "## Cross-scoring method\n",
    "\n",
    "Data preparation process is the same as the course slide<br>\n",
    "Except: <br>\n",
    "1.There is no truncation or common, rare words elimination while generating the dictionary <br\\>(Out-of-Vocabulary is also considered)<br>\n",
    "2.The error correction dictionary \"my_dict.txt\" is applied to \n",
    "fix those miscut vocabularies,<br>\n",
    "ex: [\"是\",\"不\",\"是\"]-----> [\"是不是\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\Administrator\\Desktop\\DL\\Comp1\\big5_dict.txt ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.u4db63a6c4a8ce99b1489a4d22431124c.cache\n",
      "Loading model cost 0.819 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy as np\n",
    "import jieba.posseg as pseg\n",
    "def jieba_lines(lines):\n",
    "    cut_lines = []\n",
    "    for line in lines:\n",
    "        cut_line = jieba.lcut(line)\n",
    "        cut_lines.append(cut_line)\n",
    "    return cut_lines\n",
    "jieba.set_dictionary('big5_dict.txt')\n",
    "jieba.load_userdict('my_dict.txt')\n",
    "cut_programs = np.load('cut_Programs.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training words vector using gensim's Word2Vec <br\\>\n",
    "Vector size: 150, 100 epochs, this will take about 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this will run about 40 min\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(cut_programs,\n",
    "                 size=150, #Dimensionality of the word vectors.\n",
    "                 window=5,  #Maximum distance between the current and predicted word within a sentence.\n",
    "                 min_count=0,# Ignores all words with total frequency lower than this.\n",
    "                 workers=4)\n",
    " \n",
    "\n",
    "model.train(cut_programs, total_examples=len(cut_programs), epochs=100)\n",
    "\n",
    "model.save(\"word2vec.model\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate voc_dict and cut_Questions using the same way as the course slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "voc_dict = np.load('voc_dict.npy')\n",
    "cut_ques = np.load('cut_Questions.npy');print('Ques loaded')\n",
    "model = word2vec.Word2Vec.load(\"word2vec.model\");print('vec loaded')\n",
    "hi_freq = np.load('voc_dict.npy');print('hi_freq loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create freq dictionary (key: chinese word, value: number of occurrence) <br/>\n",
    "We save the word_dict (see course slide for more details) to word_dict.pkl beforehand<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# hi_freq = np.load('word_dict.npy');print('hi_freq loaded')\n",
    "with open('word_dict.pkl', 'rb') as f:\n",
    "    hi_freq = pickle.load(f)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot((HI_MAX-tmp[:500])/HI_MAX)\n",
    "plt.show()\n",
    "w1 =  \"怎麼\"\n",
    "w2 = \"幹嘛\"\n",
    "print(hi_freq[w1])\n",
    "print(hi_freq[w2])\n",
    "np.power(model.wv.similarity(w1,w2),4)\n",
    "\n",
    "freq = {}\n",
    "FROM = 0\n",
    "TO = len(hi_freq)\n",
    "for i in range(FROM,TO):\n",
    "    freq[hi_freq[i][0]] =int(hi_freq[i][1])\n",
    "\n",
    "HI_MAX = int(hi_freq[FROM][1])\n",
    "print(HI_MAX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function could get the last sentence of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def group(seq, sep):\n",
    "    g = []\n",
    "    for el in seq:\n",
    "        if el == sep:\n",
    "            yield g\n",
    "            g = []\n",
    "            continue\n",
    "        g.append(el)\n",
    "    yield g\n",
    "\n",
    "ex = ['A', ' ', 'B' , 'C' , ' ' , 'D',' ']\n",
    "result = list(group(ex, ' '))[-2]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special chinese word remapping <br/>\n",
    "Ex: \"拿到河邊去烤一烤\"  -> \"拿到河邊去烤\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "def rep_fix(line):\n",
    "    fix_line = []\n",
    "    i=0\n",
    "    while i<len(line):\n",
    "        try:\n",
    "            if line[i+1]=='一':\n",
    "                if line[i] == line[i+2]:\n",
    "                    fix_line.append(line[i])\n",
    "                    i+=3\n",
    "                    continue\n",
    "        except:pass\n",
    "        fix_line.append(line[i])\n",
    "        i+=1\n",
    "\n",
    "    return fix_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#these words will be eliminate if it appears in the question \n",
    "stop_words =  set([' ','這','你','我','的','不','是','是不是','啦','嗎','嘛','反而','又','有沒有','耶','啊','這麼','都','了'\n",
    "                  ,'才','自己','應該','人','你還','其實','因為','該'])\n",
    "\n",
    "time_words = set(['今天','明天','後天','現在','以前'])\n",
    "\n",
    "#these words are not likely to appear in two consecutive lines\n",
    "Non2_words = set(['可是','雖然','然而','如果'])\n",
    "\n",
    "#對應句\n",
    "#If the key word appear in the last sentence of the question, listening the value word \n",
    "#Option contains the value words is much likely to be the candidate\n",
    "bonus = {'因為':['所以','就'],'就算':['還是','也是'],'雖然':['但是'],'為什麼':['因為']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The core of the scoring algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def get_score(cut_ques,debug = False):\n",
    "    score = np.zeros([6,])\n",
    "    lines = cut_ques[0]\n",
    "    d = set()\n",
    "    listen = set()\n",
    "    \n",
    "    #last sentence of the question\n",
    "    last = list(group(lines[-2], ' '))[-1]\n",
    "    \n",
    "    #因為...所以...對應句listner\n",
    "    for w in last:\n",
    "        try:\n",
    "            listen.add(bonus[w][0])\n",
    "        except:continue\n",
    "\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            if word not in stop_words:\n",
    "                d.add(word)\n",
    "    \n",
    "    #for each option\n",
    "    for i in range(1, 7):\n",
    "        line = cut_ques[i]\n",
    "        line = rep_fix(line)\n",
    "\n",
    "        for ai,a in enumerate(line):\n",
    "            #If 對應句 matched, aggravating the score\n",
    "            if a in listen: \n",
    "                score[i-1] += 0.5\n",
    "            for q in d:\n",
    "                s = 0\n",
    "                try:\n",
    "                    # power of four feature\n",
    "                    s += np.power(model.wv.similarity(a,q),4)\n",
    "                except:\n",
    "                    #If the word is not found in the dictionary, it could be rare name\n",
    "                    #If there's some option contain these name , add extra score to it\n",
    "                    if a == q:\n",
    "                        score[i-1]+= 0.4\n",
    "                    continue\n",
    "                s = min(s,0.7)\n",
    "                #Increase the similarity of nested word\n",
    "                #Ex:  \n",
    "                #\"中暑\" contains the word \"暑\"\n",
    "                for qq in q:\n",
    "                    if qq in a:\n",
    "                        s+=0.02\n",
    "                        break\n",
    "                #If question and the response is talking the same time, add an extra score\n",
    "                #else we give it negative score penalty\n",
    "                if (a in time_words) and (q in time_words):\n",
    "                    if a != q:\n",
    "                        s = -abs(s)*2               \n",
    "                    else:\n",
    "                        s = 1.0\n",
    "                #If two word are matched yet their occurrence count is more than 200 in the program\n",
    "                # 0.8 factor penalty is added\n",
    "                #If Non2_words appear consecutively, add negative score\n",
    "                elif a==q:\n",
    "                    if hi_freq[a]>200:\n",
    "                        s *=0.8\n",
    "\n",
    "                    if (a in Non2_words) and (a in lines[-2]):\n",
    "                        s = -abs(s)\n",
    "                #As for those rare words, increase it's importance by *2\n",
    "                else:\n",
    "                    if hi_freq[a]<300 and hi_freq[q] <300 :\n",
    "                        s *= 2.0\n",
    "                hi_fil  = 8000  \n",
    "                #These words are too common to be taken into account\n",
    "                #However, they could be useful sometimes, so we multiply it by 0.31\n",
    "                if hi_freq[a]>hi_fil and hi_freq[q] >hi_fil:\n",
    "                    s *= 0.3\n",
    "                #If one word is rare and the other is common, multiply it by 0.8\n",
    "                elif (hi_freq[a]>hi_fil) != (hi_freq[q]>hi_fil):\n",
    "                    s *= 0.8\n",
    "                #debug code\n",
    "                if s > 0.02 and debug:\n",
    "                    print(i-1,q,a,np.round(s,4))\n",
    "                \n",
    "                score[i-1]+= s\n",
    "            \n",
    "        \n",
    "    return score\n",
    "get_score(cut_ques[316],debug = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  jieba.possegjieba.pos  as pseg\n",
    "#for each question\n",
    "ans = np.zeros([500,])\n",
    "score = np.zeros([500,6])\n",
    "\n",
    "for q_id,question in enumerate(cut_ques):\n",
    "    score[q_id,:] = get_score(question)\n",
    "    ans[q_id] = np.argmax(score[q_id,:])\n",
    "    \n",
    "ans = ans.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "outname = 'output.csv'\n",
    "with open(outname, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Id\",\"Answer\"])\n",
    "    for i,val in enumerate(ans):\n",
    "        writer.writerow([ i, val])\n",
    "print(outname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a QA-Robot by Neural Network\n",
    "In this part, we trained a neural network model for the **QA-robot** in competition 1. This part will be organized in three parts: (a) Word embedding model, (b) generating training data and computing the features, and (c) the graph and the session of our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors\n",
    "To process the natural language sentence, we have to encode each chinese word before training the neural network model. We choose FastText as the word embedding model. All the sentences in training data were partitioned into word level by jieba, and converted into the format which can be read by gensim fasttext model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing training data as the same in the TA's example code and save in *.npy file format\n",
    "# Read in the processed and partitioned word file and convert it into the format of gensim\n",
    "import numpy as np\n",
    "cut_programs = np.load('cut_Programs.npy')\n",
    "cut_Question = np.load('cut_Questions.npy')\n",
    "print(sum([len(p) for p in cut_programs]))\n",
    "\n",
    "with open('split_word_Programs.txt', 'w', encoding='utf-8') as output:\n",
    "    for i, program in enumerate(programs):\n",
    "        episodes = len(program)\n",
    "        print('Processing the %d programs' % i)\n",
    "        for episode in range(episodes):\n",
    "            for line in cut_programs[i][episode]:\n",
    "                line_space = \" \".join(line)\n",
    "                output.write(line_space)\n",
    "                output.write('\\n')\n",
    "\n",
    "with open('split_word_Questions.txt', 'w', encoding='utf-8') as output:\n",
    "    n = len(questions)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(6):\n",
    "            for line in cut_Question[i][j]:\n",
    "                line_space = \" \".join(line)\n",
    "                output.write(line_space)\n",
    "                output.write('\\n')\n",
    "                \n",
    "# Train a fasttext model\n",
    "from gensim.models import word2vec, doc2vec, FastText\n",
    "import numpy as np\n",
    "\n",
    "vec_size = 128\n",
    "sentences = word2vec.LineSentence('split_word_Programs.txt')\n",
    "model = FastText(sentences, size=vec_size, window=5,min_count=1)\n",
    "model.save(\"fasttext\"+str(vec_size)+\".model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training data\n",
    "To train a QA-Robot model by the corpus from the normal programs, the data should be transformed into 1 question and 6 answer candidates format to. We define two consecutive sentences are a pair of correct question and answer. With exclusively and randomly selected 5 sentences, a complete QA pair is formed. Due to the tediousness of the lengthy codes, the codes are not shown in this report. Please refer to `gen_data.py` and `gen_data_infer.py` for the details of implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the features\n",
    "The most important problem of the feature extraction is how to deal with the variable length sentence. Therefore, we have tried 3 types of feature to unify the word vectors of different sentences:\n",
    "1. Padding the word vectors to the length which is equals to the maximun legnth of the sentence in the training data. The data are padded with **wrapping values** instead of **zeros** for avoiding the numerical imbalance. The dimension of this feature is [*MAX_LEN*, *VECTOR_SIZE*].\n",
    "2. Averaging all the word vectors (along the axis of words). The dimension of this feature is [*VECTOR_SIZE*].\n",
    "3. Calculating the standard deviation from all the word vectors of the sentence. The dimension of this feature is also [*VECTOR_SIZE*].\n",
    "4. Calculating the Word Mover's Distance (WMD) between the question and the answer candidate. The dimension of this feature is [*1*].\n",
    "This codes are part of `gen_data.py` and `gen_data_infer.py`. Note that `self.w2vembedd2` is a FastText model and `self.w2vembedd` is a Doc2Vec model which was not used in the final version.\n",
    "\n",
    "Several combinations of the features and corresponding were tested, more details and results will be discuessed in the next parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(self, sentence_idx):\n",
    "    sentvec = sent = 0\n",
    "    word_c2 = word_c = 0\n",
    "    sent_sq = 0\n",
    "    wv_seq = np.empty([0,VECTOR_SIZE])\n",
    "    for i, w in enumerate(self.sentences[sentence_idx]):\n",
    "        if MAX_LEN <= word_c:\n",
    "\n",
    "            break\n",
    "        if w.isalnum():\n",
    "            try:\n",
    "                wv_seq = np.vstack([wv_seq, self.w2vembedd2.wv[[w]]])\n",
    "                word_c += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "            try:\n",
    "                sentvec += self.w2vembedd.wv[w]\n",
    "                #sent_var += self.w2vembedd2.wv[w]**2\n",
    "                word_c2 += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "    if word_c > 0:\n",
    "        sent = np.mean(wv_seq,0)\n",
    "        sent_var = np.std(wv_seq,0)\n",
    "    if word_c2 > 0:\n",
    "        sentvec = sentvec / word_c2\n",
    "    return np.concatenate([sentvec,sent,sent_var], 0), wv_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The graph and the session of our neural network\n",
    "We assume that different types of feature preserve its characteristic. The following figure is the overal scheme of our model:\n",
    "\n",
    "|  ![alt text](fig1.png \"Title\")  | ![alt text](fig2.png \"Title\")  |\n",
    "|--------|--------|\n",
    "\n",
    "![alt text](fig3.png \"Title\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question(q_sentence, training_flag):\n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "        scale=1e-5, scope=None)\n",
    "    frame_out = tf.layers.conv2d(q_sentence, 256, [1,1], kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.nn.relu)\n",
    "    frame_att = tf.layers.conv2d(q_sentence, 256, [1,1], kernel_initializer=tf.contrib.layers.xavier_initializer(\n",
    "        ), activation=tf.nn.sigmoid, kernel_regularizer=l1_regularizer)\n",
    "    frame_out *= frame_att\n",
    "    pool_size = frame_out.get_shape().as_list()[2]\n",
    "    frame_out = tf.layers.average_pooling2d(\n",
    "        frame_out, pool_size=[1,pool_size], strides=1)\n",
    "    return frame_out\n",
    "\n",
    "\n",
    "def answer(a_sentence, training_flag):\n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "        scale=1e-5, scope=None)\n",
    "    frame_out = tf.layers.conv2d(a_sentence, 256, [1,1], kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.nn.relu)\n",
    "    frame_att = tf.layers.conv2d(a_sentence, 256, [1,1], kernel_initializer=tf.contrib.layers.xavier_initializer(\n",
    "        ), activation=tf.nn.sigmoid, kernel_regularizer=l1_regularizer)\n",
    "    frame_out *= frame_att\n",
    "    pool_size = frame_out.get_shape().as_list()[2]\n",
    "    frame_out = tf.layers.average_pooling2d(\n",
    "        frame_out, pool_size=[1,pool_size], strides=1)\n",
    "    return frame_out\n",
    "\n",
    "\n",
    "def similar(logits_q, logits_a, training_flag, p,is_batchnorm=False,name='similarity'):\n",
    "\n",
    "    qa_pair = tf.concat([logits_q * logits_a,logits_q**2,logits_a**2], -1)\n",
    "    qa_pair = denselayer(qa_pair, 256, tf.nn.relu, is_batchnorm=is_batchnorm,\n",
    "                         is_dropout=True, prob=p, is_training=training_flag, repeat=1)\n",
    "    fc_out = denselayer(qa_pair, 64, tf.nn.relu, is_batchnorm=is_batchnorm,\n",
    "                        is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "    return fc_out\n",
    "\n",
    "def selection(simi_all, training_flag, p):\n",
    "    qa_pair = simi_all\n",
    "    qa_pair = denselayer(qa_pair, 64, tf.nn.selu, is_batchnorm=False,\n",
    "                         is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "    fc_out = denselayer(qa_pair, 6, None, is_batchnorm=False,\n",
    "                        is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "    return fc_out\n",
    "\n",
    "dim = 1024\n",
    "def dim_reduce(x_input, name, window=1,dim = 1024):\n",
    "    with tf.variable_scope(name):\n",
    "        l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "            scale=1e-5, scope=None)\n",
    "        x = tf.layers.conv1d(x_input, dim, [\n",
    "                             window], kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.nn.selu)\n",
    "        \n",
    "        xa = tf.layers.conv1d(x, dim, [\n",
    "                             1], kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.nn.selu)\n",
    "        x_att = tf.layers.conv1d(x, dim, [1], kernel_initializer=tf.contrib.layers.xavier_initializer(\n",
    "        ), activation=tf.nn.sigmoid, kernel_regularizer=l1_regularizer)\n",
    "        x = xa * x_att\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "    model = models.Word2Vec.load('doc2vec128.model')\n",
    "    model2 = models.FastText.load('fasttext128.model')\n",
    "    #model.init_sims(replace=True)\n",
    "    model2.init_sims(replace=True)\n",
    "    train_dataset = gen_dataset.dataset(w2v=model, w2v2=model2, val_ratio=0.2)\n",
    "    test_dataset = gen_dataset_infer.dataset(\n",
    "        w2v=model, w2v2=model2, val_ratio=0)\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x_input = tf.placeholder(\n",
    "            tf.float32, [None, 7, gen_dataset.VECTOR_SIZE * 3])\n",
    "        x_seq = tf.placeholder(\n",
    "            tf.float32, [None, 7, gen_dataset.MAX_LEN, gen_dataset.VECTOR_SIZE])\n",
    "        y_label = tf.placeholder(tf.float32, [None, 6])\n",
    "        sim_tensor = tf.placeholder(tf.float32, [None, 6])\n",
    "        training_flag = tf.placeholder(tf.bool)\n",
    "        prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        #module for the all WVs\n",
    "        simi_mat_red = tf.reshape(question(x_seq[:,0:1,:,:], training_flag) * answer(x_seq[:,1:,:,:], training_flag),[-1,256])\n",
    "        \n",
    "        #module for the avg. of WVs\n",
    "        x1q = dim_reduce(\n",
    "            x_input[:, 0:1, gen_dataset.VECTOR_SIZE:gen_dataset.VECTOR_SIZE * 2], 'red_1_q')\n",
    "        x1a = dim_reduce(\n",
    "            x_input[:, 1:, gen_dataset.VECTOR_SIZE:gen_dataset.VECTOR_SIZE * 2], 'red_1_a')\n",
    "        \n",
    "        #module for the std. of WVs\n",
    "        x3q = dim_reduce(\n",
    "            x_input[:, 0:1, gen_dataset.VECTOR_SIZE*2:gen_dataset.VECTOR_SIZE * 3], 'red_3_q')\n",
    "        x3a = dim_reduce(\n",
    "            x_input[:, 1:, gen_dataset.VECTOR_SIZE*2:gen_dataset.VECTOR_SIZE * 3], 'red_s3_a')\n",
    "\n",
    "        x = tf.concat([x1q,x1a], 1)\n",
    "        x3 = tf.concat([x3q,x3a], 1)\n",
    "\n",
    "        x_question = tf.reshape(tf.tile(x[:, 0:1, :],[1,6,1]),[-1,x.shape[-1]])\n",
    "        x_ans = tf.reshape(x[:, 1:, :],[-1,x.shape[-1]])\n",
    "        x3_question = tf.reshape(tf.tile(x3[:, 0:1, :],[1,6,1]),[-1,x.shape[-1]])\n",
    "        x3_ans = tf.reshape(x3[:, 1:, :],[-1,x.shape[-1]])\n",
    "\n",
    "        a = similar(x_question, x_ans, training_flag, prob)\n",
    "        b = similar(x3_question, x3_ans, training_flag, prob,is_batchnorm=True)\n",
    "\n",
    "        sim_exp = tf.reshape(sim_tensor, [-1,1])\n",
    "        simi_all = tf.concat([simi_mat_red,sim_exp], -1)\n",
    "        simi_single = tf.reshape(simi_all, [-1, simi_all.shape[-1]])\n",
    "        single_out = denselayer(simi_single, 64, tf.nn.selu, is_batchnorm=False,\n",
    "                         is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "        single_out = denselayer(single_out, 1, None, is_batchnorm=False,\n",
    "                         is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "        simi_all = tf.reshape(simi_all, [-1, simi_all.shape[-1] * 6])\n",
    "\n",
    "        out = selection(simi_all, training_flag, prob)\n",
    "        sigmoid_out = tf.nn.sigmoid(out)\n",
    "        pred = tf.argmax(softmax_out,1)\n",
    "        reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss_single = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=single_out,  labels=tf.reshape(y_label,[-1,1])))\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=out,  labels=y_label))\n",
    "        loss = tf.reduce_sum(reg_ws) + loss_single + loss\n",
    "\n",
    "        acc = tf.equal(tf.argmax(y_label, 1), pred)\n",
    "        acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        writer = tf.summary.FileWriter(graph_dir, tf.get_default_graph())\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    best_acc = 0\n",
    "    early_count = 0\n",
    "    ans = np.load('label.npy')\n",
    "    with tf.Session(config=config, graph=g) as sess:\n",
    "        writer = tf.summary.FileWriter(graph_dir, graph=sess.graph)\n",
    "        sess.run(init)\n",
    "        for i in range(3000):\n",
    "            s, label, sim, wv_seq, end_flag = train_dataset.getbatch(\n",
    "                mode='train', batch_size=batch_size)\n",
    "            if end_flag != gen_dataset.IS_EPOCH_END:\n",
    "                lo, _, a = sess.run([loss, optimizer, acc], feed_dict={\n",
    "                                    x_input: s, y_label: label, sim_tensor: sim, x_seq:wv_seq, training_flag: True, prob: 0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary results\n",
    "\n",
    "We found that too small batch size leads the training instability (size < 128). Therefore, we tried 512, 1024, 2048 as the batch size. For selecting input feature, we first several model by only one type of feature. Surprisingly, the average and stadard deviation of the word vectors give a better results. We further try the different combination of the feature to train different model.\n",
    "\n",
    "In the end of the competition, we selected 2 results for the deep learning methods. One is from the best parameters, another is from voting by different training results under different parameters.\n",
    "\n",
    "| |  Best  | Voting  |\n",
    "|----|--------|--------|\n",
    "|Public score|0.612|0.648|\n",
    "|Private score|0.688|0.692|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: ensemble\n",
    "We had implemented two models. Select the answer with the higher confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"ensemble.csv\") \n",
    "data2 = pd.read_csv(\"output.csv\") \n",
    "jo_pred = data.values[:,1]\n",
    "my_pred = data2.values[:,1]\n",
    "\n",
    "k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "\n",
    "\n",
    "k_change = []\n",
    "final = []\n",
    "\n",
    "ma,de,av = [1.0,0.2,0.11] # max,difference of top 2 score, average\n",
    "\n",
    "for i in range(500):\n",
    "    ss = np.sort(score[i])\n",
    "    delta = ss[-1] - ss[-2]\n",
    "    ave = np.mean(ss)\n",
    "\n",
    "    if ss[-1] <ma and delta <de or ave < av :\n",
    "        final.append(jo_pred[i])\n",
    "        if my_pred[i] !=jo_pred[i] and jo_pred[i] == label[i]:\n",
    "            k_change.append(i)\n",
    "    else:\n",
    "        final.append(my_pred[i])\n",
    "if (sum(label == final)/500) > 0.73:\n",
    "    print(ma,de,av,len(k_change),sum(label == final)/500)\n",
    "    print(k_change)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
